{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI.0.SVM.Accuracies.Predictions.ipynb","provenance":[],"authorship_tag":"ABX9TyNVDo7jQo1LoL948XFpPoxO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"G9WuMLF-h4Kt"},"source":["# open and read Cassie's data\r\n","import random\r\n","from matplotlib import pyplot as plt\r\n","import numpy as np\r\n","import math\r\n","from sklearn.svm import SVC\r\n","#from sklearn.datasets import make_classification\r\n","from sklearn.model_selection import GridSearchCV\r\n","import warnings\r\n","import pandas as pd\r\n","from sklearn.preprocessing import MinMaxScaler as Scaler\r\n","scaler = Scaler()\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn import metrics\r\n","warnings.simplefilter(action='ignore', category=Warning)\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive')\r\n","inputfile = \"/content/gdrive/My Drive/Landslide Files/Copy of ls_data_SVM.1_28_2021.csv\" \r\n","\r\n","### dataset specifies any feature to be dropped from the anaylsis \r\n","dataset = 'TEST' # \r\n","# log10(surface.area)\r\n","\r\n","with open(inputfile, 'r') as f:\r\n","  data = f.read()\r\n","  print(repr(data)[:100])\r\n","  print(repr(data)[-100:])\r\n","  print(len(data))\r\n","f.close()\r\n","lines = data.split('\\n')\r\n","lines.pop()\r\n","print(len(lines))\r\n","print(lines[0])\r\n","print(lines[1])\r\n","\r\n","headings = lines[0].split(',')\r\n","print(headings)\r\n","\r\n","table =[]\r\n","for x in range(1, len(lines)):\r\n","  row = lines[x].split(',')\r\n","  if row[len(row)-1] != 'NA':\r\n","    for y in range(1, len(row)): row[y] = float(row[y])\r\n","    table.append([ row[0], row[15], row[7], row[8], row[9], row[3], math.log10(row[16]),row[5], row[17]/row[16], row[20] ])\r\n","\"\"\" \r\n","landslides_polygons$id,mean.tch,sd.tch,mean.slope,mean.tri,tri.10m,elev,elev.km,ls.northing,ls.easting,insol.dsm,insol.kw,insol,agb,year,age,surface.area,perimeter,elev.mean.tch,max.tch.buf,rem\r\n","  0                     1        2      3          4        5      6     7       8           9          10        11       12   13  14   15   16          17         18           19          20\r\n","ID, age, elev.km, ls.northing, ls.easting, mean.slope, log10(surface.area), tri.10m, pa.ratio, rem \r\n","0   1    2        3            4           5           6                    8        9         10       \r\n","\"\"\"\r\n","### make young and old datasets.  The young data will be split into train&test for SVC, then that model will be applied to old_df, and result placed in oldraw\r\n","youngraw, oldraw =[], [] \r\n","for x in range(len(table)):\r\n","  if table[x][1] <= 5:\r\n","    youngraw.append(table[x])\r\n","  else: \r\n","    table[x].append(0)\r\n","    oldraw.append(table[x])\r\n","### make young_df, which will be split and used for SVC below\r\n","new_headings = [ 'ID', 'age', 'elev.km', 'ls.northing', 'ls.easting', 'mean.slope', 'log10(surface.area)',  'tri.10m', 'pa.ratio', 'rem']\r\n","young_df = pd.DataFrame(youngraw, columns=new_headings)\r\n","\r\n","### make old_df and preprocess it.  It will be ready for predictions\r\n","old_headings = [ 'ID', 'age', 'elev.km', 'ls.northing', 'ls.easting', 'mean.slope', 'log10(surface.area)',  'tri.10m', 'pa.ratio', 'old.rem', 'svmrem' ]\r\n","old_df = pd.DataFrame(oldraw, columns = old_headings)\r\n","old_df_labels = old_df[\"old.rem\"].copy()\r\n","old_df = old_df.drop(\"old.rem\", axis=1)\r\n","old_df = old_df.drop(\"svmrem\", axis=1)\r\n","old_df = old_df.drop(\"age\", axis=1)\r\n","old_df = old_df.drop(\"ID\", axis=1)\r\n","#old_df = old_df.drop('log10(surface.area)', axis=1)\r\n","#old_df = old_df.drop('ls.easting', axis=1)\r\n","#old_df = old_df.drop('ls.northing', axis=1)\r\n","\r\n","scaler.fit(old_df) \r\n","old_df_scaled = scaler.transform(old_df)\r\n","\r\n","### oldraw has all the data for old sites, old_df_scaled is ready for predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2bv00SRh65j"},"source":["# pandas treatments and SVM routines\r\n","\r\n","from sklearn.model_selection import train_test_split\r\n","import statistics as stats\r\n","\r\n","scorefile = \"/content/gdrive/My Drive/ML Files/Accuracy.Precision.Recall.\" +dataset+ \".csv\" \r\n","predictionfile =  \"/content/gdrive/My Drive/ML Files/Predictions.\" +dataset+ \".csv\"\r\n","start =0\r\n","finish = 100 \r\n","if start ==0:\r\n","  with open(scorefile, 'w') as e: \r\n","    e.close()\r\n","  with open(predictionfile, 'w') as d: \r\n","    for x in range(len(oldraw)): \r\n","      d.write('0,')\r\n","  d.close()\r\n","\r\n","accuracies, recall, precision =[], [], []\r\n","for a in range(start, finish):  # for drop: 'log(surface.area),pa.ratio', we have 0-2 and 86-99.  Need 3-86\r\n","  #if a% 10 ==0: print('Working on', a)\r\n","  train_set, test_set = train_test_split(young_df, test_size=0.5, random_state = a )  \r\n","### assign rem as value to be predicted, drop columns to be excluded \r\n","  train_set_labels = train_set[\"rem\"].copy()\r\n","  train_set = train_set.drop(\"rem\", axis=1)\r\n","  train_set = train_set.drop(\"age\", axis=1)\r\n","  train_set = train_set.drop(\"ID\", axis=1)\r\n","  #train_set = train_set.drop(\"log10(surface.area)\", axis=1)\r\n","  #train_set = train_set.drop('ls.easting', axis =1)\r\n","  #train_set = train_set.drop('ls.northing', axis =1)\r\n","\r\n","  test_set_labels = test_set[\"rem\"].copy()\r\n","  test_set = test_set.drop(\"rem\", axis=1)\r\n","  test_set = test_set.drop(\"age\", axis=1)\r\n","  test_set = test_set.drop(\"ID\", axis=1)\r\n","  #test_set = test_set.drop(\"log10(surface.area)\", axis=1)\r\n","  #test_set = test_set.drop('ls.easting', axis =1)\r\n","  #test_set = test_set.drop('ls.northing', axis =1)\r\n","\r\n","### the SVM routines\r\n","  scaler.fit(train_set) \r\n","  train_set_scaled = scaler.transform(train_set)\r\n","  test_set_scaled = scaler.transform(test_set)\r\n","  clf = SVC()  # SVC, NuSVC\r\n","  param_grid = {\r\n","        'C': [0.1, 1.0, 10.0, 50.0], \r\n","        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\r\n","        'shrinking': [True, False],\r\n","        'degree': [1,2,3,4,5,6], \r\n","        'gamma': ['auto', 'scale', 1, 0.1, 0.01],\r\n","        'coef0': [0.0, 0.1, 0.5]  }\r\n","\r\n","  grid_search = GridSearchCV(\r\n","          clf, param_grid, cv=10, scoring='accuracy')\r\n","  grid_search.fit(train_set_scaled, train_set_labels)\r\n","  X = np.append(train_set_scaled, test_set_scaled, axis=0)\r\n","  y = np.append(train_set_labels, test_set_labels, axis=0)\r\n","  CLF = clf.fit(X, y)\r\n","### prediction statistics on test set\r\n","  predicted_y = clf.predict(test_set_scaled)\r\n","  RECALL = metrics.recall_score(test_set_labels, predicted_y)\r\n","  PRECISION = metrics.precision_score(test_set_labels, predicted_y)\r\n","  ACCURACY = metrics.accuracy_score(test_set_labels, predicted_y)\r\n","  print(a, grid_search.best_score_, ACCURACY, PRECISION, RECALL)\r\n","  accuracies.append(ACCURACY)\r\n","  recall.append(RECALL)\r\n","  precision.append(PRECISION)\r\n","  with open(scorefile, 'a') as g: g.write(str(a) +','+ str(ACCURACY) +',' +str(PRECISION)+ ',' + str(RECALL) +'\\n') \r\n","  g.close()\r\n","### predictions for old sites\r\n","  prediction = clf.predict(old_df_scaled)\r\n","  with open(predictionfile, 'r') as e:\r\n","    predraw = e.read()\r\n","  e.close()\r\n","  preds = predraw.split(',')\r\n","  preds.pop()\r\n","  for x in range(len(prediction)):\r\n","    if prediction[x] == 1: \r\n","      preds[x] = str(int(preds[x]) +1) \r\n","  with open(predictionfile, 'w') as e:\r\n","    for y in range(len(preds)): e.write(preds[y] + ',')\r\n","  e.close()\r\n","print('Dataset=', dataset, 'Mean accuracy =', stats.mean(accuracies), ', sem =', stats.stdev(accuracies) / math.sqrt(len(accuracies) ) )\r\n","print('Done.')\r\n"],"execution_count":null,"outputs":[]}]}